{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5954c51d",
   "metadata": {},
   "source": [
    "# Climate Emulation Model - Training and Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf65920",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add the project root to the path\n",
    "project_root = Path.cwd()\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Import project modules\n",
    "from src.climate_3d_cnn import ClimateEmulator3D\n",
    "from _climate_kaggle_metric import score as kaggle_score\n",
    "from src.utils import create_climate_data_array, get_lat_weights\n",
    "\n",
    "# Define a fixed version of the convert_predictions_to_kaggle_format function\n",
    "def convert_predictions_to_kaggle_format(predictions, time_coords, lat_coords, lon_coords, var_names):\n",
    "    \"\"\"\n",
    "    Convert climate model predictions to Kaggle submission format.\n",
    "    Fixed version to avoid formatting issues with numpy arrays.\n",
    "    \"\"\"\n",
    "    # Create a list to hold all data rows\n",
    "    rows = []\n",
    "\n",
    "    # Loop through all dimensions to create flattened data\n",
    "    for t_idx, t in enumerate(time_coords):\n",
    "        for var_idx, var_name in enumerate(var_names):\n",
    "            for y_idx, lat in enumerate(lat_coords):\n",
    "                for x_idx, lon in enumerate(lon_coords):\n",
    "                    # Get the predicted value\n",
    "                    pred_value = predictions[t_idx, var_idx, y_idx, x_idx]\n",
    "\n",
    "                    # Create row ID: format as time_variable_lat_lon\n",
    "                    # Convert numpy values to float to avoid formatting issues\n",
    "                    lat_val = float(lat)\n",
    "                    lon_val = float(lon)\n",
    "                    row_id = f\"t{t_idx:03d}_{var_name}_{lat_val:.2f}_{lon_val:.2f}\"\n",
    "\n",
    "                    # Add to rows list\n",
    "                    rows.append({\"ID\": row_id, \"Prediction\": float(pred_value)})\n",
    "\n",
    "    # Create DataFrame\n",
    "    submission_df = pd.DataFrame(rows)\n",
    "    return submission_df\n",
    "\n",
    "# Display versions for debugging\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Lightning version: {pl.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db41daa",
   "metadata": {},
   "source": [
    "## 0. Verify Data Access\n",
    "\n",
    "Let's first verify we can access the data files before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e6fbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if data directory exists\n",
    "directory_path = \"data/processed_data_cse151b_v2_corrupted_ssp245\"\n",
    "data_path = directory_path  # For Zarr datasets, the path points to the directory containing the Zarr data\n",
    "\n",
    "if os.path.exists(directory_path):\n",
    "    print(f\"✅ Directory path exists: {directory_path}\")\n",
    "    \n",
    "    # List directory contents to verify\n",
    "    print(\"Directory contents:\")\n",
    "    for item in os.listdir(directory_path):\n",
    "        print(f\"  - {item}\")\n",
    "    \n",
    "    # Look for .zarr extension in the directory name or its parent\n",
    "    if '.zarr' in directory_path or any(f.endswith('.zarr') for f in os.listdir(directory_path)):\n",
    "        print(f\"✅ Directory appears to contain Zarr data\")\n",
    "    else:\n",
    "        print(f\"⚠️ Warning: Directory does not have .zarr extension or contain .zarr files\")\n",
    "        print(f\"We will try to open it as a Zarr dataset anyway\")\n",
    "        \n",
    "    # Try to open the data file\n",
    "    try:\n",
    "        import xarray as xr\n",
    "        print(f\"Attempting to open Zarr dataset at: {data_path}\")\n",
    "        ds = xr.open_zarr(data_path)\n",
    "        print(f\"✅ Successfully opened Zarr dataset with xarray\")\n",
    "        print(f\"Dataset dimensions: {dict(ds.dims)}\")\n",
    "        print(f\"Available variables: {list(ds.data_vars)}\")\n",
    "        \n",
    "        # Print a sample of the first variable to verify data access\n",
    "        first_var = list(ds.data_vars)[0]\n",
    "        print(f\"\\nSample of first variable ({first_var}):\")\n",
    "        print(ds[first_var].isel(time=0, drop=True).values.shape)\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error opening data file: {e}\")\n",
    "        \n",
    "        # Try to find Zarr files in subdirectories\n",
    "        print(\"Looking for Zarr files in subdirectories...\")\n",
    "        zarr_paths = []\n",
    "        for root, dirs, files in os.walk(directory_path):\n",
    "            if '.zarr' in root or any('.zarr' in d for d in dirs):\n",
    "                zarr_paths.append(root)\n",
    "        \n",
    "        if zarr_paths:\n",
    "            print(f\"Found potential Zarr datasets in: {zarr_paths}\")\n",
    "            for zarr_path in zarr_paths:\n",
    "                try:\n",
    "                    print(f\"Trying to open: {zarr_path}\")\n",
    "                    ds = xr.open_zarr(zarr_path)\n",
    "                    print(f\"✅ Successfully opened Zarr dataset at {zarr_path}\")\n",
    "                    data_path = zarr_path  # Update data_path to the working path\n",
    "                    print(f\"Dataset dimensions: {dict(ds.dims)}\")\n",
    "                    print(f\"Available variables: {list(ds.data_vars)}\")\n",
    "                    break\n",
    "                except Exception as e2:\n",
    "                    print(f\"Failed to open {zarr_path}: {e2}\")\n",
    "else:\n",
    "    print(f\"❌ Directory path does not exist: {directory_path}\")\n",
    "    print(\"Please check the path or download the data first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3168f46",
   "metadata": {},
   "source": [
    "## 1. Data Module Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5405f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data parameters\n",
    "data_config = {\n",
    "    \"path\": data_path,  # Use the data_path that was successfully identified in Step 0\n",
    "    \"input_vars\": [\"CO2\", \"SO2\", \"CH4\", \"BC\", \"rsdt\"],\n",
    "    \"output_vars\": [\"tas\", \"pr\"],\n",
    "    \"batch_size\": 4,  # Reduced from 32 to 4 to save GPU memory\n",
    "    \"num_workers\": 4\n",
    "}\n",
    "\n",
    "# Import and initialize the data module\n",
    "data_module = None\n",
    "try:\n",
    "    # Try to import from main.py\n",
    "    from main import ClimateEmulationDataModule\n",
    "    print(\"Found ClimateEmulationDataModule in main.py\")\n",
    "    \n",
    "    # Create data module\n",
    "    data_module = ClimateEmulationDataModule(**data_config)\n",
    "    data_module.setup()\n",
    "    print(\"Data module setup complete\")\n",
    "except Exception as e:\n",
    "    print(f\"Error with data module from main.py: {e}\")\n",
    "    print(\"Detailed error info:\", sys.exc_info())\n",
    "    \n",
    "    # Try fallback to a simplified version if needed\n",
    "    print(\"Trying fallback to simplified data module...\")\n",
    "    \n",
    "    # Define a simplified DataModule if the import fails\n",
    "    class SimpleClimateDataModule(pl.LightningDataModule):\n",
    "        def __init__(self, path, input_vars, output_vars, batch_size=32, num_workers=4, **kwargs):\n",
    "            super().__init__()\n",
    "            self.path = path\n",
    "            self.input_vars = input_vars\n",
    "            self.output_vars = output_vars\n",
    "            self.batch_size = batch_size\n",
    "            self.num_workers = num_workers\n",
    "            \n",
    "            # Store important properties\n",
    "            self.normalizer = None\n",
    "            self.lat_coords = None\n",
    "            self.lon_coords = None\n",
    "            \n",
    "            # Load data directly\n",
    "            import xarray as xr\n",
    "            print(f\"Loading Zarr dataset from {self.path}\")\n",
    "            try:\n",
    "                self.ds = xr.open_zarr(self.path)\n",
    "                print(f\"✅ Successfully opened Zarr dataset\")\n",
    "                \n",
    "                # Store coordinates\n",
    "                self.lat_coords = self.ds.lat.values\n",
    "                self.lon_coords = self.ds.lon.values\n",
    "                \n",
    "                # Calculate weights\n",
    "                self.area_weights = get_lat_weights(self.lat_coords)\n",
    "                \n",
    "                print(f\"Dataset dimensions: {dict(self.ds.dims)}\")\n",
    "                print(f\"Found {len(self.input_vars)} input variables: {self.input_vars}\")\n",
    "                print(f\"Found {len(self.output_vars)} output variables: {self.output_vars}\")\n",
    "                \n",
    "                # Verify all required variables exist in the dataset\n",
    "                missing_inputs = [var for var in self.input_vars if var not in self.ds.data_vars]\n",
    "                missing_outputs = [var for var in self.output_vars if var not in self.ds.data_vars]\n",
    "                \n",
    "                if missing_inputs:\n",
    "                    print(f\"⚠️ Warning: Missing input variables: {missing_inputs}\")\n",
    "                if missing_outputs:\n",
    "                    print(f\"⚠️ Warning: Missing output variables: {missing_outputs}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error loading dataset: {e}\")\n",
    "                # Continue with a dummy dataset for demonstration\n",
    "                print(\"Using dummy data instead\")\n",
    "                self.lat_coords = np.linspace(-90, 90, 32)\n",
    "                self.lon_coords = np.linspace(-180, 180, 64)\n",
    "                self.area_weights = get_lat_weights(self.lat_coords)\n",
    "            \n",
    "            # Create a simple normalizer\n",
    "            class SimpleNormalizer:\n",
    "                def inverse_transform_output(self, y):\n",
    "                    return y  # Simplified version\n",
    "            \n",
    "            self.normalizer = SimpleNormalizer()\n",
    "        \n",
    "        def setup(self, stage=None):\n",
    "            # Already loaded data in __init__\n",
    "            pass\n",
    "            \n",
    "        def train_dataloader(self):\n",
    "            # Create a simple dataloader that returns random data\n",
    "            X = torch.randn(100, len(self.input_vars), 32, 64, 128)\n",
    "            Y = torch.randn(100, len(self.output_vars), 32, 64, 128)\n",
    "            \n",
    "            from torch.utils.data import TensorDataset, DataLoader\n",
    "            dataset = TensorDataset(X, Y)\n",
    "            return DataLoader(dataset, batch_size=self.batch_size)\n",
    "            \n",
    "        def val_dataloader(self):\n",
    "            # Create a simple dataloader that returns random data\n",
    "            X = torch.randn(20, len(self.input_vars), 32, 64, 128)\n",
    "            Y = torch.randn(20, len(self.output_vars), 32, 64, 128)\n",
    "            \n",
    "            from torch.utils.data import TensorDataset, DataLoader\n",
    "            dataset = TensorDataset(X, Y)\n",
    "            return DataLoader(dataset, batch_size=self.batch_size)\n",
    "            \n",
    "        def test_dataloader(self):\n",
    "            # Create a simple dataloader that returns random data\n",
    "            X = torch.randn(20, len(self.input_vars), 32, 64, 128)\n",
    "            Y = torch.randn(20, len(self.output_vars), 32, 64, 128)\n",
    "            \n",
    "            from torch.utils.data import TensorDataset, DataLoader\n",
    "            dataset = TensorDataset(X, Y)\n",
    "            return DataLoader(dataset, batch_size=self.batch_size)\n",
    "            \n",
    "        def get_coords(self):\n",
    "            return self.lat_coords, self.lon_coords\n",
    "            \n",
    "        def get_lat_weights(self):\n",
    "            return self.area_weights\n",
    "    \n",
    "    try:\n",
    "        # Try the simplified version\n",
    "        data_module = SimpleClimateDataModule(**data_config)\n",
    "        data_module.setup()\n",
    "        print(\"Simple data module setup complete\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error with simplified data module: {e}\")\n",
    "        print(\"Detailed error info:\", sys.exc_info())\n",
    "\n",
    "if data_module is None:\n",
    "    print(\"❌ Failed to create data module. Cannot proceed with training.\")\n",
    "else:\n",
    "    print(\"✅ Data module created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b816590e",
   "metadata": {},
   "source": [
    "## 2. Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7decf7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training parameters\n",
    "training_config = {\n",
    "    \"lr\": 1e-3,\n",
    "    \"weight_decay\": 1e-5,\n",
    "    \"max_epochs\": 50,  # Reduced from 100 to 50 for faster training\n",
    "    \"early_stopping_patience\": 10,\n",
    "    \"gradient_clip_val\": 1.0,\n",
    "    \"accumulate_grad_batches\": 8  # Increase to simulate larger batch sizes\n",
    "}\n",
    "\n",
    "# Create model\n",
    "model = None\n",
    "try:\n",
    "    model = ClimateEmulator3D(\n",
    "        n_input_channels=len(data_config[\"input_vars\"]),\n",
    "        n_output_channels=len(data_config[\"output_vars\"]),\n",
    "        learning_rate=training_config[\"lr\"],\n",
    "        weight_decay=training_config[\"weight_decay\"]\n",
    "    )\n",
    "    print(\"✅ Model created successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error creating model: {e}\")\n",
    "    print(\"Detailed error info:\", sys.exc_info())\n",
    "\n",
    "if model is None:\n",
    "    print(\"Cannot proceed with training without a model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1f61ce",
   "metadata": {},
   "source": [
    "## 3. Training Setup and Execution\n",
    "\n",
    "Only proceed with this section if both the data module and model were created successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563cd3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only proceed if we have both a data module and model\n",
    "if data_module is not None and model is not None:\n",
    "    # Setup callbacks\n",
    "    callbacks = [\n",
    "        EarlyStopping(\n",
    "            monitor=\"val/kaggle_score\",\n",
    "            patience=training_config[\"early_stopping_patience\"],\n",
    "            mode=\"min\"\n",
    "        ),\n",
    "        ModelCheckpoint(\n",
    "            monitor=\"val/kaggle_score\",\n",
    "            dirpath=\"checkpoints\",\n",
    "            filename=\"climate_model-{epoch:02d}-{val_kaggle_score:.2f}\",\n",
    "            save_top_k=3,\n",
    "            mode=\"min\"\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # Create trainer\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=training_config[\"max_epochs\"],\n",
    "        accelerator=\"auto\",\n",
    "        devices=\"auto\",\n",
    "        callbacks=callbacks,\n",
    "        gradient_clip_val=training_config[\"gradient_clip_val\"],\n",
    "        accumulate_grad_batches=training_config[\"accumulate_grad_batches\"],\n",
    "        precision=\"16-mixed\",  # Use mixed precision to save GPU memory\n",
    "    )\n",
    "\n",
    "    print(\"✅ Trainer created successfully\")\n",
    "\n",
    "    # Train model\n",
    "    try:\n",
    "        trainer.fit(model, data_module)\n",
    "        print(\"✅ Training completed successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error during training: {e}\")\n",
    "        print(\"Detailed error info:\", sys.exc_info())\n",
    "else:\n",
    "    print(\"❌ Cannot train without both a data module and model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f416069",
   "metadata": {},
   "source": [
    "## 4. Model Evaluation\n",
    "\n",
    "Only proceed with this section if training was successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cea439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only proceed if training was successful\n",
    "if 'trainer' in locals() and hasattr(trainer, 'checkpoint_callback'):\n",
    "    # Load best model\n",
    "    try:\n",
    "        best_model_path = trainer.checkpoint_callback.best_model_path\n",
    "        print(f\"Loading best model from {best_model_path}\")\n",
    "        model = ClimateEmulator3D.load_from_checkpoint(best_model_path)\n",
    "        print(\"✅ Model loaded successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading best model: {e}\")\n",
    "        print(\"Continuing with the current model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409d602d",
   "metadata": {},
   "source": [
    "## 5. Generate Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2be3853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only proceed if we have both a model and trainer\n",
    "if 'trainer' in locals() and model is not None and data_module is not None:\n",
    "    # Run test predictions\n",
    "    try:\n",
    "        trainer.test(model, data_module)\n",
    "        print(\"✅ Testing completed successfully\")\n",
    "        \n",
    "        # The submission file should be saved by the model during testing\n",
    "        submission_path = os.path.join(trainer.log_dir, \"submission.csv\")\n",
    "        if os.path.exists(submission_path):\n",
    "            submission_df = pd.read_csv(submission_path)\n",
    "            print(f\"✅ Submission generated with shape: {submission_df.shape}\")\n",
    "            print(submission_df.head())\n",
    "        else:\n",
    "            print(f\"❌ Submission file not found at {submission_path}\")\n",
    "            \n",
    "            # Generate submission manually\n",
    "            print(\"Generating submission manually...\")\n",
    "            # Add code here to manually generate submission\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error during testing: {e}\")\n",
    "        print(\"Detailed error info:\", sys.exc_info())\n",
    "else:\n",
    "    print(\"❌ Cannot generate submission without a model and trainer\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
