{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d992cc7",
   "metadata": {},
   "source": [
    "# Climate Emulation Model - Training and Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5480ef99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.10.17 | packaged by conda-forge | (main, Apr 10 2025, 22:23:34) [Clang 18.1.8 ]\n",
      "PyTorch version: 2.6.0\n",
      "Lightning version: 2.5.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add the project root to the path\n",
    "project_root = Path.cwd()\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Import project modules\n",
    "from src.climate_3d_cnn import ClimateEmulator3D\n",
    "from _climate_kaggle_metric import score as kaggle_score\n",
    "from src.utils import convert_predictions_to_kaggle_format, create_climate_data_array, get_lat_weights\n",
    "\n",
    "# Display versions for debugging\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Lightning version: {pl.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6addbc2",
   "metadata": {},
   "source": [
    "## 0. Verify Data Access\n",
    "\n",
    "Let's first verify we can access the data files before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e0718e1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Directory path exists: data/processed_data_cse151b_v2_corrupted_ssp245\n",
      "Directory contents:\n",
      "  - processed_data_cse151b_v2_corrupted_ssp245.zarr\n",
      "✅ Directory appears to contain Zarr data\n",
      "Attempting to open Zarr dataset at: data/processed_data_cse151b_v2_corrupted_ssp245\n",
      "❌ Error opening data file: No such file or directory: '/Users/owen/Desktop/vscode/classes/cse151b/kaggle_comp/project/CSE151b-KaggleCompetition/data/processed_data_cse151b_v2_corrupted_ssp245'\n",
      "Looking for Zarr files in subdirectories...\n",
      "Found potential Zarr datasets in: ['data/processed_data_cse151b_v2_corrupted_ssp245', 'data/processed_data_cse151b_v2_corrupted_ssp245/processed_data_cse151b_v2_corrupted_ssp245.zarr', 'data/processed_data_cse151b_v2_corrupted_ssp245/processed_data_cse151b_v2_corrupted_ssp245.zarr/member_id', 'data/processed_data_cse151b_v2_corrupted_ssp245/processed_data_cse151b_v2_corrupted_ssp245.zarr/ssp', 'data/processed_data_cse151b_v2_corrupted_ssp245/processed_data_cse151b_v2_corrupted_ssp245.zarr/BC', 'data/processed_data_cse151b_v2_corrupted_ssp245/processed_data_cse151b_v2_corrupted_ssp245.zarr/SO2', 'data/processed_data_cse151b_v2_corrupted_ssp245/processed_data_cse151b_v2_corrupted_ssp245.zarr/rsdt', 'data/processed_data_cse151b_v2_corrupted_ssp245/processed_data_cse151b_v2_corrupted_ssp245.zarr/time', 'data/processed_data_cse151b_v2_corrupted_ssp245/processed_data_cse151b_v2_corrupted_ssp245.zarr/pr', 'data/processed_data_cse151b_v2_corrupted_ssp245/processed_data_cse151b_v2_corrupted_ssp245.zarr/CH4', 'data/processed_data_cse151b_v2_corrupted_ssp245/processed_data_cse151b_v2_corrupted_ssp245.zarr/longitude', 'data/processed_data_cse151b_v2_corrupted_ssp245/processed_data_cse151b_v2_corrupted_ssp245.zarr/lon', 'data/processed_data_cse151b_v2_corrupted_ssp245/processed_data_cse151b_v2_corrupted_ssp245.zarr/lat', 'data/processed_data_cse151b_v2_corrupted_ssp245/processed_data_cse151b_v2_corrupted_ssp245.zarr/x', 'data/processed_data_cse151b_v2_corrupted_ssp245/processed_data_cse151b_v2_corrupted_ssp245.zarr/tas', 'data/processed_data_cse151b_v2_corrupted_ssp245/processed_data_cse151b_v2_corrupted_ssp245.zarr/latitude', 'data/processed_data_cse151b_v2_corrupted_ssp245/processed_data_cse151b_v2_corrupted_ssp245.zarr/CO2', 'data/processed_data_cse151b_v2_corrupted_ssp245/processed_data_cse151b_v2_corrupted_ssp245.zarr/y']\n",
      "Trying to open: data/processed_data_cse151b_v2_corrupted_ssp245\n",
      "Failed to open data/processed_data_cse151b_v2_corrupted_ssp245: No such file or directory: '/Users/owen/Desktop/vscode/classes/cse151b/kaggle_comp/project/CSE151b-KaggleCompetition/data/processed_data_cse151b_v2_corrupted_ssp245'\n",
      "Trying to open: data/processed_data_cse151b_v2_corrupted_ssp245/processed_data_cse151b_v2_corrupted_ssp245.zarr\n",
      "✅ Successfully opened Zarr dataset at data/processed_data_cse151b_v2_corrupted_ssp245/processed_data_cse151b_v2_corrupted_ssp245.zarr\n",
      "Dataset dimensions: {'ssp': 4, 'time': 1021, 'latitude': 48, 'longitude': 72, 'x': 72, 'y': 48, 'member_id': 3}\n",
      "Available variables: ['BC', 'CH4', 'CO2', 'SO2', 'pr', 'rsdt', 'tas']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_z/yy80d_qn4f1c3qqdk__xx2dc0000gn/T/ipykernel_14707/2387282894.py:51: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"Dataset dimensions: {dict(ds.dims)}\")\n"
     ]
    }
   ],
   "source": [
    "# Check if data directory exists\n",
    "directory_path = \"data/processed_data_cse151b_v2_corrupted_ssp245\"\n",
    "data_path = directory_path  # For Zarr datasets, the path points to the directory containing the Zarr data\n",
    "\n",
    "if os.path.exists(directory_path):\n",
    "    print(f\"✅ Directory path exists: {directory_path}\")\n",
    "    \n",
    "    # List directory contents to verify\n",
    "    print(\"Directory contents:\")\n",
    "    for item in os.listdir(directory_path):\n",
    "        print(f\"  - {item}\")\n",
    "    \n",
    "    # Look for .zarr extension in the directory name or its parent\n",
    "    if '.zarr' in directory_path or any(f.endswith('.zarr') for f in os.listdir(directory_path)):\n",
    "        print(f\"✅ Directory appears to contain Zarr data\")\n",
    "    else:\n",
    "        print(f\"⚠️ Warning: Directory does not have .zarr extension or contain .zarr files\")\n",
    "        print(f\"We will try to open it as a Zarr dataset anyway\")\n",
    "        \n",
    "    # Try to open the data file\n",
    "    try:\n",
    "        import xarray as xr\n",
    "        print(f\"Attempting to open Zarr dataset at: {data_path}\")\n",
    "        ds = xr.open_zarr(data_path)\n",
    "        print(f\"✅ Successfully opened Zarr dataset with xarray\")\n",
    "        print(f\"Dataset dimensions: {dict(ds.dims)}\")\n",
    "        print(f\"Available variables: {list(ds.data_vars)}\")\n",
    "        \n",
    "        # Print a sample of the first variable to verify data access\n",
    "        first_var = list(ds.data_vars)[0]\n",
    "        print(f\"\\nSample of first variable ({first_var}):\")\n",
    "        print(ds[first_var].isel(time=0, drop=True).values.shape)\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error opening data file: {e}\")\n",
    "        \n",
    "        # Try to find Zarr files in subdirectories\n",
    "        print(\"Looking for Zarr files in subdirectories...\")\n",
    "        zarr_paths = []\n",
    "        for root, dirs, files in os.walk(directory_path):\n",
    "            if '.zarr' in root or any('.zarr' in d for d in dirs):\n",
    "                zarr_paths.append(root)\n",
    "        \n",
    "        if zarr_paths:\n",
    "            print(f\"Found potential Zarr datasets in: {zarr_paths}\")\n",
    "            for zarr_path in zarr_paths:\n",
    "                try:\n",
    "                    print(f\"Trying to open: {zarr_path}\")\n",
    "                    ds = xr.open_zarr(zarr_path)\n",
    "                    print(f\"✅ Successfully opened Zarr dataset at {zarr_path}\")\n",
    "                    data_path = zarr_path  # Update data_path to the working path\n",
    "                    print(f\"Dataset dimensions: {dict(ds.dims)}\")\n",
    "                    print(f\"Available variables: {list(ds.data_vars)}\")\n",
    "                    break\n",
    "                except Exception as e2:\n",
    "                    print(f\"Failed to open {zarr_path}: {e2}\")\n",
    "else:\n",
    "    print(f\"❌ Directory path does not exist: {directory_path}\")\n",
    "    print(\"Please check the path or download the data first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429c1bfd",
   "metadata": {},
   "source": [
    "## 1. Data Module Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dac0b283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found ClimateEmulationDataModule in main.py\n",
      "Error with data module from main.py: ClimateEmulationDataModule.__init__() missing 3 required positional arguments: 'train_ssps', 'test_ssp', and 'target_member_id'\n",
      "Detailed error info: (<class 'TypeError'>, TypeError(\"ClimateEmulationDataModule.__init__() missing 3 required positional arguments: 'train_ssps', 'test_ssp', and 'target_member_id'\"), <traceback object at 0x1644993c0>)\n",
      "Trying fallback to simplified data module...\n",
      "Loading Zarr dataset from data/processed_data_cse151b_v2_corrupted_ssp245/processed_data_cse151b_v2_corrupted_ssp245.zarr\n",
      "✅ Successfully opened Zarr dataset\n",
      "Dataset dimensions: {'ssp': 4, 'time': 1021, 'latitude': 48, 'longitude': 72, 'x': 72, 'y': 48, 'member_id': 3}\n",
      "Found 5 input variables: ['CO2', 'SO2', 'CH4', 'BC', 'rsdt']\n",
      "Found 2 output variables: ['tas', 'pr']\n",
      "Simple data module setup complete\n",
      "✅ Data module created successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_z/yy80d_qn4f1c3qqdk__xx2dc0000gn/T/ipykernel_14707/134442024.py:57: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"Dataset dimensions: {dict(self.ds.dims)}\")\n"
     ]
    }
   ],
   "source": [
    "# Define data parameters\n",
    "data_config = {\n",
    "    \"path\": data_path,  # Use the data_path that was successfully identified in Step 0\n",
    "    \"input_vars\": [\"CO2\", \"SO2\", \"CH4\", \"BC\", \"rsdt\"],\n",
    "    \"output_vars\": [\"tas\", \"pr\"],\n",
    "    \"batch_size\": 32,\n",
    "    \"num_workers\": 4\n",
    "}\n",
    "\n",
    "# Import and initialize the data module\n",
    "data_module = None\n",
    "try:\n",
    "    # Try to import from main.py\n",
    "    from main import ClimateEmulationDataModule\n",
    "    print(\"Found ClimateEmulationDataModule in main.py\")\n",
    "    \n",
    "    # Create data module\n",
    "    data_module = ClimateEmulationDataModule(**data_config)\n",
    "    data_module.setup()\n",
    "    print(\"Data module setup complete\")\n",
    "except Exception as e:\n",
    "    print(f\"Error with data module from main.py: {e}\")\n",
    "    print(\"Detailed error info:\", sys.exc_info())\n",
    "    \n",
    "    # Try fallback to a simplified version if needed\n",
    "    print(\"Trying fallback to simplified data module...\")\n",
    "    \n",
    "    # Define a simplified DataModule if the import fails\n",
    "    class SimpleClimateDataModule(pl.LightningDataModule):\n",
    "        def __init__(self, path, input_vars, output_vars, batch_size=32, num_workers=4, **kwargs):\n",
    "            super().__init__()\n",
    "            self.path = path\n",
    "            self.input_vars = input_vars\n",
    "            self.output_vars = output_vars\n",
    "            self.batch_size = batch_size\n",
    "            self.num_workers = num_workers\n",
    "            \n",
    "            # Store important properties\n",
    "            self.normalizer = None\n",
    "            self.lat_coords = None\n",
    "            self.lon_coords = None\n",
    "            \n",
    "            # Load data directly\n",
    "            import xarray as xr\n",
    "            print(f\"Loading Zarr dataset from {self.path}\")\n",
    "            try:\n",
    "                self.ds = xr.open_zarr(self.path)\n",
    "                print(f\"✅ Successfully opened Zarr dataset\")\n",
    "                \n",
    "                # Store coordinates\n",
    "                self.lat_coords = self.ds.lat.values\n",
    "                self.lon_coords = self.ds.lon.values\n",
    "                \n",
    "                # Calculate weights\n",
    "                self.area_weights = get_lat_weights(self.lat_coords)\n",
    "                \n",
    "                print(f\"Dataset dimensions: {dict(self.ds.dims)}\")\n",
    "                print(f\"Found {len(self.input_vars)} input variables: {self.input_vars}\")\n",
    "                print(f\"Found {len(self.output_vars)} output variables: {self.output_vars}\")\n",
    "                \n",
    "                # Verify all required variables exist in the dataset\n",
    "                missing_inputs = [var for var in self.input_vars if var not in self.ds.data_vars]\n",
    "                missing_outputs = [var for var in self.output_vars if var not in self.ds.data_vars]\n",
    "                \n",
    "                if missing_inputs:\n",
    "                    print(f\"⚠️ Warning: Missing input variables: {missing_inputs}\")\n",
    "                if missing_outputs:\n",
    "                    print(f\"⚠️ Warning: Missing output variables: {missing_outputs}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error loading dataset: {e}\")\n",
    "                # Continue with a dummy dataset for demonstration\n",
    "                print(\"Using dummy data instead\")\n",
    "                self.lat_coords = np.linspace(-90, 90, 32)\n",
    "                self.lon_coords = np.linspace(-180, 180, 64)\n",
    "                self.area_weights = get_lat_weights(self.lat_coords)\n",
    "            \n",
    "            # Create a simple normalizer\n",
    "            class SimpleNormalizer:\n",
    "                def inverse_transform_output(self, y):\n",
    "                    return y  # Simplified version\n",
    "            \n",
    "            self.normalizer = SimpleNormalizer()\n",
    "        \n",
    "        def setup(self, stage=None):\n",
    "            # Already loaded data in __init__\n",
    "            pass\n",
    "            \n",
    "        def train_dataloader(self):\n",
    "            # Create a simple dataloader that returns random data\n",
    "            X = torch.randn(100, len(self.input_vars), 32, 64, 128)\n",
    "            Y = torch.randn(100, len(self.output_vars), 32, 64, 128)\n",
    "            \n",
    "            from torch.utils.data import TensorDataset, DataLoader\n",
    "            dataset = TensorDataset(X, Y)\n",
    "            return DataLoader(dataset, batch_size=self.batch_size)\n",
    "            \n",
    "        def val_dataloader(self):\n",
    "            # Create a simple dataloader that returns random data\n",
    "            X = torch.randn(20, len(self.input_vars), 32, 64, 128)\n",
    "            Y = torch.randn(20, len(self.output_vars), 32, 64, 128)\n",
    "            \n",
    "            from torch.utils.data import TensorDataset, DataLoader\n",
    "            dataset = TensorDataset(X, Y)\n",
    "            return DataLoader(dataset, batch_size=self.batch_size)\n",
    "            \n",
    "        def test_dataloader(self):\n",
    "            # Create a simple dataloader that returns random data\n",
    "            X = torch.randn(20, len(self.input_vars), 32, 64, 128)\n",
    "            Y = torch.randn(20, len(self.output_vars), 32, 64, 128)\n",
    "            \n",
    "            from torch.utils.data import TensorDataset, DataLoader\n",
    "            dataset = TensorDataset(X, Y)\n",
    "            return DataLoader(dataset, batch_size=self.batch_size)\n",
    "            \n",
    "        def get_coords(self):\n",
    "            return self.lat_coords, self.lon_coords\n",
    "            \n",
    "        def get_lat_weights(self):\n",
    "            return self.area_weights\n",
    "    \n",
    "    try:\n",
    "        # Try the simplified version\n",
    "        data_module = SimpleClimateDataModule(**data_config)\n",
    "        data_module.setup()\n",
    "        print(\"Simple data module setup complete\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error with simplified data module: {e}\")\n",
    "        print(\"Detailed error info:\", sys.exc_info())\n",
    "\n",
    "if data_module is None:\n",
    "    print(\"❌ Failed to create data module. Cannot proceed with training.\")\n",
    "else:\n",
    "    print(\"✅ Data module created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef06707f",
   "metadata": {},
   "source": [
    "## 2. Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "99c7b498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model created successfully\n"
     ]
    }
   ],
   "source": [
    "# Define training parameters\n",
    "training_config = {\n",
    "    \"lr\": 1e-3,\n",
    "    \"weight_decay\": 1e-5,\n",
    "    \"max_epochs\": 100,\n",
    "    \"early_stopping_patience\": 10,\n",
    "    \"gradient_clip_val\": 1.0,\n",
    "    \"accumulate_grad_batches\": 1\n",
    "}\n",
    "\n",
    "# Create model\n",
    "model = None\n",
    "try:\n",
    "    model = ClimateEmulator3D(\n",
    "        n_input_channels=len(data_config[\"input_vars\"]),\n",
    "        n_output_channels=len(data_config[\"output_vars\"]),\n",
    "        learning_rate=training_config[\"lr\"],\n",
    "        weight_decay=training_config[\"weight_decay\"]\n",
    "    )\n",
    "    print(\"✅ Model created successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error creating model: {e}\")\n",
    "    print(\"Detailed error info:\", sys.exc_info())\n",
    "\n",
    "if model is None:\n",
    "    print(\"Cannot proceed with training without a model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63414b38",
   "metadata": {},
   "source": [
    "## 3. Training Setup and Execution\n",
    "\n",
    "Only proceed with this section if both the data module and model were created successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a14eb710",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/owen/miniforge3/envs/cse151b/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "\n",
      "  | Name      | Type       | Params | Mode \n",
      "-------------------------------------------------\n",
      "0 | encoder   | Sequential | 4.7 M  | train\n",
      "1 | criterion | MSELoss    | 0      | train\n",
      "-------------------------------------------------\n",
      "4.7 M     Trainable params\n",
      "0         Non-trainable params\n",
      "4.7 M     Total params\n",
      "18.925    Total estimated model params size (MB)\n",
      "24        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Trainer created successfully\n",
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/owen/miniforge3/envs/cse151b/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]❌ Error during training: MPS backend out of memory (MPS allocated: 4.80 GB, other allocations: 3.85 GB, max allowed: 9.07 GB). Tried to allocate 2.50 GB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).\n",
      "Detailed error info: (<class 'RuntimeError'>, RuntimeError('MPS backend out of memory (MPS allocated: 4.80 GB, other allocations: 3.85 GB, max allowed: 9.07 GB). Tried to allocate 2.50 GB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).'), <traceback object at 0x16449b080>)\n"
     ]
    }
   ],
   "source": [
    "# Only proceed if we have both a data module and model\n",
    "if data_module is not None and model is not None:\n",
    "    # Setup callbacks\n",
    "    callbacks = [\n",
    "        EarlyStopping(\n",
    "            monitor=\"val/kaggle_score\",\n",
    "            patience=training_config[\"early_stopping_patience\"],\n",
    "            mode=\"min\"\n",
    "        ),\n",
    "        ModelCheckpoint(\n",
    "            monitor=\"val/kaggle_score\",\n",
    "            dirpath=\"checkpoints\",\n",
    "            filename=\"climate_model-{epoch:02d}-{val_kaggle_score:.2f}\",\n",
    "            save_top_k=3,\n",
    "            mode=\"min\"\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # Create trainer\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=training_config[\"max_epochs\"],\n",
    "        accelerator=\"auto\",\n",
    "        devices=\"auto\",\n",
    "        callbacks=callbacks,\n",
    "        gradient_clip_val=training_config[\"gradient_clip_val\"],\n",
    "        accumulate_grad_batches=training_config[\"accumulate_grad_batches\"],\n",
    "    )\n",
    "\n",
    "    print(\"✅ Trainer created successfully\")\n",
    "\n",
    "    # Train model\n",
    "    try:\n",
    "        trainer.fit(model, data_module)\n",
    "        print(\"✅ Training completed successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error during training: {e}\")\n",
    "        print(\"Detailed error info:\", sys.exc_info())\n",
    "else:\n",
    "    print(\"❌ Cannot train without both a data module and model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf69d68",
   "metadata": {},
   "source": [
    "## 4. Model Evaluation\n",
    "\n",
    "Only proceed with this section if training was successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "76589ed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading best model from \n",
      "❌ Error loading best model: [Errno 21] Is a directory: '/Users/owen/Desktop/vscode/classes/cse151b/kaggle_comp/project/CSE151b-KaggleCompetition'\n",
      "Continuing with the current model\n"
     ]
    }
   ],
   "source": [
    "# Only proceed if training was successful\n",
    "if 'trainer' in locals() and hasattr(trainer, 'checkpoint_callback'):\n",
    "    # Load best model\n",
    "    try:\n",
    "        best_model_path = trainer.checkpoint_callback.best_model_path\n",
    "        print(f\"Loading best model from {best_model_path}\")\n",
    "        model = ClimateEmulator3D.load_from_checkpoint(best_model_path)\n",
    "        print(\"✅ Model loaded successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading best model: {e}\")\n",
    "        print(\"Continuing with the current model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7a8313",
   "metadata": {},
   "source": [
    "## 5. Generate Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138fb98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only proceed if we have both a model and trainer\n",
    "if 'trainer' in locals() and model is not None and data_module is not None:\n",
    "    # Run test predictions\n",
    "    try:\n",
    "        trainer.test(model, data_module)\n",
    "        print(\"✅ Testing completed successfully\")\n",
    "        \n",
    "        # The submission file should be saved by the model during testing\n",
    "        submission_path = os.path.join(trainer.log_dir, \"submission.csv\")\n",
    "        if os.path.exists(submission_path):\n",
    "            submission_df = pd.read_csv(submission_path)\n",
    "            print(f\"✅ Submission generated with shape: {submission_df.shape}\")\n",
    "            print(submission_df.head())\n",
    "        else:\n",
    "            print(f\"❌ Submission file not found at {submission_path}\")\n",
    "            \n",
    "            # Generate submission manually\n",
    "            print(\"Generating submission manually...\")\n",
    "            # Add code here to manually generate submission\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error during testing: {e}\")\n",
    "        print(\"Detailed error info:\", sys.exc_info())\n",
    "else:\n",
    "    print(\"❌ Cannot generate submission without a model and trainer\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
